# Please make a copy of this file and call it config.yaml. Make sure not to share it on GitHub, as you will need to
# add sensitive information, e.g. API token.
# Use this newly created copy to adjust all configurations.
API_token: # your token here
    IBMQ_API_token: ""
    dwave_API_token: ""

# choose backend
backend: "sqa"
    # (string) the backend algorithm to be used. options are:
    # "classical", "sqa", "dwave-tabu", "dwave-greedy", "dwave-hybrid", "dwave-qpu", "dwave-read-qpu",
    # "pypsa-glpk", "pypsa-fico", "qaoa"

# all configuration data of the QUBO model of the network
ising_interface:
    # values for specifiying network to qubit abstraction
    formulation: "customsplit"
        # (string) name of the function how to model lines with qubits
        # choose between binarysplit, fullsplit, customsplit, cutpowersoftwo
        #
    # each followung entry is the configuration data for one QUBI subproblem
    kirchhoff:
        # Adding Kirchhoff subproblem to QUBO
        scale_factor: 1.0
        # (float) weight of the kirchhoff subproblem in the QUBO
    marginal_cost:
        # Adding Marginal cost optimization in the QUBO
        formulation: "GlobalCostSquare"
            # (string) name of the class/procedure how to encode the marginal cost optimization
            # choose between GlobalCostSquare, GlobalCostSquareWithSlack, MarginalAsPenalty, LocalMarginalEstimationDistance
        ### these values only pertain the formulation *WithSlack
        slack_type: "binary_power"
            # this determines the method how slack variables weights are generated. this will generate powers of 2 as weights
        slack_scale: 0.1
            # (float) scalar factor for all slack variable weights
        slack_size: 4
            # (int) number of slack variables to be used
        ### end *WithSlack
        ### these values specifiy how costs are estimated if a formulation needs it (all but MarginalAsPenalty)
        scale_factor: 0.6
             # (float) weight of the marginal cost optimization in the QUBO
        offset_estimation_factor: 1.225
             # (float) factor that determines estimate of marginal cost. A Factor 1.0 corresponds to the estimation 
             # obtained by adding the most efficient generators to the solution and cap it when the total load has been met
             # this method disregerads the topology of the network and is a lower boundary of the cost
        estimated_cost_factor: 1.0
             # (float) TODO: remove this option in code
        offset_build_factor: 1.0
             # (float) TODO: remove this option in code
        ### end estimation config

    # TODO not implemented in code yey
    # 
    min_up_down_time:
        min_up_down_factor: 1.0


qaoa_backend:
    shots: 200
        # (int) the number of shots
        # max 20,000
    simulate: True
        # (boolean) use a simulator or a QPU
        # if True: use simulator;
        # if False: use real IBMQ QPU (IBMQ API token required)
    noise: False
        # (boolean) simulate with or without noise (only used, when simulate is True)
        # if True: use simulator with noise model (IBMQ API token required);
        # if False: use simulator without noise model
    simulator: "aer_simulator"
        # (string) the qiskit simulator to be used (only used, when simulate is True)
        # choose between "qasm_simulator", "aer_simulator", "statevector_simulator" 
        # and "aer_simulator_statevector"
        
    # supervisior determines how angles are chosen at the beginning of one experiment
    # There are two possible supervisor right now. they are grouped with the parameter
    # for choosing initial angles because the type varies for different supervisors
    # supervisior_type: "RandomOrFixed"
    # supervisior_type: "GridSearch"
        # (string) how to choose inital angles
        # choose between "RandomOrFixed" and "GridSearch"
    max_iter: 10
        # (int) maximum number iterations of the classical optimizer
    repetitions: 5
        # (int) number of experiment repetitions
    classical_optimizer: "COBYLA"
        # (string) the classical optimizer to be used
        # choose between "SPSA", "COBYLA" and "ADAM"

    ### "RandomOrFixed" config guess
    # uses a list to initializes angles in one experiment, substituting the string "rand" by a random number
    supervisior_type: "RandomOrFixed"
    initial_guess: ["rand", "rand"]
        # (list of float or "rand") the initial guess for the classical optimizer and "RandomOrFixed" supervisor
    range: 2
        # (float) range of a random guess for the angle. A range of 1 will result in random guesses
        # in the intervall [-1,1]. Only applicable for random guesses
        # has to be an even number of parameters starting with beta, then gamma, then beta again and so on...;
        # use "rand" if a random initial guess should be used for this parameter. 
        
#   ### "GridSearch" config guess
#   # GridSearch takes a list of grids which are dictionary that describe the initial angles 
#   # of the layers. A grid dictionary containts the following keys
#        lower_bound: -3
#           # (float) lowest point of the angles 
#        upper_bound: 3
#           # (float)  highest point of angles 
#        num_gridpoints: 1
#           # (int) number of angles to try for the problem hamiltonian. Raises an error if fewer
#           # than 1 are selected. If excactly one is selected, the lower_bound is the chosen grid point
    supervisior_type: "GridSearch"
    initial_guess: 
    - lower_bound: 2
      upper_bound: 2
      num_gridpoints: 3
    - lower_bound: 1
      upper_bound: 1
      num_gridpoints: 3

#   # It also supports default values for all grids given in the dictionary which are used as fallback values
#   # The fallback values are given by the dictionary `default_grid`. The default values for that one are
#   # the following ones
#    default_grid:
#       lower_bound: -3
#       upper_bound: 3
#       num_gridpoints: 1


sqa_backend:
    # seed: 10
        # (int)
    # schedule rules can be read in the platform description/sqa repo
    transverse_field_schedule: "[8.0,0.0]"
        # (string) transvere field schedule of the simulation
    temperature_schedule: "[0.1,iF,0.0001]"
        # (string) temperature schedule of the simultion
    trotter_slices: 500
        # (int) number of trotter slices to be used.
        # more trotter slices make a more precies discretization of the quantum state and
        # increases solution quality and run time
    optimization_cycles: 300
        # (int) number of discrete time steps to model continous evolution
        # more cycles improve quality of solution and run time. This falls off rather quickly


dwave_backend:
    strategy: "lowest_energy"
        # (string)
        # choose between "lowest_energy and "closest_sample"
    postprocess: "flow"
        # (string) determines how to improve qpu result. only flow optimization
        # is possible
    timeout: 100
        # (int) number of seconds that an embedding into working graph can be calculated
       
    ### only necessary for solver dwave-qpu 
    sample_origin: "infoNocost_220124cost5input_10_0_20.nc_300_200_fullsplit_60_1"
        # (string) filename of the saved sample result file if old result is reused
        # choose between absolute filepath of the sample or "cloud" (possibly receive a json via PlanQK-Platform?)

    # D-Wave annealer config, see https://docs.ocean.dwavesys.com/en/stable/overview/qpu.html
    annealing_time: 1
        # (int)
    num_reads: 1
        # (int)
    chain_strength: 1
        # (int)
    programming_thermalization: 1
        # (int)
    readout_thermalization: 1
        # (int)


pypsa_backend:
    solver_name: "glpk"
        # (string) name of the MILP solver
        # no other solver support yet
    timeout: 10
        # (int) how many second the MILP is allowed to calculate a solution. after this time, an (non-optimal) solution
        # will be returned
        
# This can also hold any of the above information. Solver specific configuration data in ${SOLVER}Backend
# will be copied into this field and then be used
backend_config:
    #
